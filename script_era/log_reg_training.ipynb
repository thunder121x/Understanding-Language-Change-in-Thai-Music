{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508abb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL ERAS: ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 1970s\n",
      "========================================\n",
      "Positive available: 64361\n",
      "Negative sizes: {'1980s': np.int64(95967), '1990s': np.int64(219135), '2000s': np.int64(443960), '2010s': np.int64(1676867), '2020s': np.int64(725925)}\n",
      "Sampling positives: 64361\n",
      "Total negatives: 64361\n",
      "Negative per class: {'1980s': 12873, '1990s': 12872, '2000s': 12872, '2010s': 12872, '2020s': 12872}\n",
      "Sampled positives: 64361\n",
      "Sampled negatives: 64361\n",
      "Final combined dataset size: 128722\n",
      "Label balance:\n",
      " is_1970s\n",
      "1    64361\n",
      "0    64361\n",
      "Name: count, dtype: int64\n",
      "Train: 104264 | Val: 11585 | Test: 12873\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 1980s\n",
      "========================================\n",
      "Positive available: 95967\n",
      "Negative sizes: {'1970s': np.int64(64361), '1990s': np.int64(219135), '2000s': np.int64(443960), '2010s': np.int64(1676867), '2020s': np.int64(725925)}\n",
      "Sampling positives: 95967\n",
      "Total negatives: 95967\n",
      "Negative per class: {'1970s': 19194, '1990s': 19194, '2000s': 19193, '2010s': 19193, '2020s': 19193}\n",
      "Sampled positives: 95967\n",
      "Sampled negatives: 95967\n",
      "Final combined dataset size: 191934\n",
      "Label balance:\n",
      " is_1980s\n",
      "1    95967\n",
      "0    95967\n",
      "Name: count, dtype: int64\n",
      "Train: 155466 | Val: 17274 | Test: 19194\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 1990s\n",
      "========================================\n",
      "Positive available: 219135\n",
      "Negative sizes: {'1970s': np.int64(64361), '1980s': np.int64(95967), '2000s': np.int64(443960), '2010s': np.int64(1676867), '2020s': np.int64(725925)}\n",
      "Sampling positives: 200000\n",
      "Total negatives: 200000\n",
      "Negative per class: {'1970s': 40000, '1980s': 40000, '2000s': 40000, '2010s': 40000, '2020s': 40000}\n",
      "Sampled positives: 200000\n",
      "Sampled negatives: 200000\n",
      "Final combined dataset size: 400000\n",
      "Label balance:\n",
      " is_1990s\n",
      "1    200000\n",
      "0    200000\n",
      "Name: count, dtype: int64\n",
      "Train: 324000 | Val: 36000 | Test: 40000\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 2000s\n",
      "========================================\n",
      "Positive available: 443960\n",
      "Negative sizes: {'1970s': np.int64(64361), '1980s': np.int64(95967), '1990s': np.int64(219135), '2010s': np.int64(1676867), '2020s': np.int64(725925)}\n",
      "Sampling positives: 200000\n",
      "Total negatives: 200000\n",
      "Negative per class: {'1970s': 40000, '1980s': 40000, '1990s': 40000, '2010s': 40000, '2020s': 40000}\n",
      "Sampled positives: 200000\n",
      "Sampled negatives: 200000\n",
      "Final combined dataset size: 400000\n",
      "Label balance:\n",
      " is_2000s\n",
      "1    200000\n",
      "0    200000\n",
      "Name: count, dtype: int64\n",
      "Train: 324000 | Val: 36000 | Test: 40000\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 2010s\n",
      "========================================\n",
      "Positive available: 1676867\n",
      "Negative sizes: {'1970s': np.int64(64361), '1980s': np.int64(95967), '1990s': np.int64(219135), '2000s': np.int64(443960), '2020s': np.int64(725925)}\n",
      "Sampling positives: 200000\n",
      "Total negatives: 200000\n",
      "Negative per class: {'1970s': 40000, '1980s': 40000, '1990s': 40000, '2000s': 40000, '2020s': 40000}\n",
      "Sampled positives: 200000\n",
      "Sampled negatives: 200000\n",
      "Final combined dataset size: 400000\n",
      "Label balance:\n",
      " is_2010s\n",
      "1    200000\n",
      "0    200000\n",
      "Name: count, dtype: int64\n",
      "Train: 324000 | Val: 36000 | Test: 40000\n",
      "\n",
      "========================================\n",
      "BUILDING DATASET FOR ERA = 2020s\n",
      "========================================\n",
      "Positive available: 725925\n",
      "Negative sizes: {'1970s': np.int64(64361), '1980s': np.int64(95967), '1990s': np.int64(219135), '2000s': np.int64(443960), '2010s': np.int64(1676867)}\n",
      "Sampling positives: 200000\n",
      "Total negatives: 200000\n",
      "Negative per class: {'1970s': 40000, '1980s': 40000, '1990s': 40000, '2000s': 40000, '2010s': 40000}\n",
      "Sampled positives: 200000\n",
      "Sampled negatives: 200000\n",
      "Final combined dataset size: 400000\n",
      "Label balance:\n",
      " is_2020s\n",
      "1    200000\n",
      "0    200000\n",
      "Name: count, dtype: int64\n",
      "Train: 324000 | Val: 36000 | Test: 40000\n",
      "\n",
      "All binary datasets created successfully with balanced pos/neg & fair per-era negatives!\n"
     ]
    }
   ],
   "source": [
    "# build_binary_era_datasets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RAW_PATH = \"../datasets_old/song_lyrics_map_era.csv\"\n",
    "OUTPUT_DIR = \"../binary_datasets_thunder\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "ALL_ERAS = sorted(df[\"song_era\"].unique())\n",
    "print(\"ALL ERAS:\", ALL_ERAS)\n",
    "\n",
    "# Global hard cap for positives & negatives per binary dataset\n",
    "MAX_TOTAL_LIMIT = 200_000\n",
    "\n",
    "\n",
    "def compute_balanced_samples(\n",
    "    pos_count: int,\n",
    "    negative_sizes: dict,\n",
    "    max_total_limit: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    pos_count          = number of items in target class\n",
    "    negative_sizes     = dict of other_eras -> size\n",
    "    max_total_limit    = optional hard cap (e.g. 200k) applied to pos & neg\n",
    "    \"\"\"\n",
    "    # Average size across other classes\n",
    "    other_counts = list(negative_sizes.values())\n",
    "    max_neg_per_class = sum(other_counts) / len(other_counts)\n",
    "\n",
    "    # Initial target for negatives = min(pos, avg_of_others)\n",
    "    neg_total_target = min(pos_count, int(max_neg_per_class))\n",
    "\n",
    "    # Can't sample more than we actually have in all negatives combined\n",
    "    max_possible_neg = sum(other_counts)\n",
    "    neg_total_target = min(neg_total_target, max_possible_neg)\n",
    "\n",
    "    # Apply global max cap (if any)\n",
    "    if max_total_limit is not None:\n",
    "        neg_total_target = min(neg_total_target, max_total_limit)\n",
    "\n",
    "    # Also cap positives to this target so pos/neg are equal\n",
    "    pos_samples = min(pos_count, neg_total_target)\n",
    "    neg_total_target = pos_samples  # enforce balance\n",
    "\n",
    "    if neg_total_target == 0:\n",
    "        return {\n",
    "            \"pos_samples\": 0,\n",
    "            \"neg_total\": 0,\n",
    "            \"neg_per_class\": {era: 0 for era in negative_sizes},\n",
    "        }\n",
    "\n",
    "    num_classes = len(negative_sizes)\n",
    "    base_each = neg_total_target // num_classes\n",
    "    leftover = neg_total_target - base_each * num_classes\n",
    "\n",
    "    distribution = {era: 0 for era in negative_sizes}\n",
    "    free_slots = 0\n",
    "\n",
    "    # First pass: give base_each to each class (capped by availability)\n",
    "    for era, size in negative_sizes.items():\n",
    "        take = min(size, base_each)\n",
    "        distribution[era] = take\n",
    "        free_slots += (base_each - take)\n",
    "\n",
    "    # Add leftover into free_slots pool\n",
    "    free_slots += leftover\n",
    "\n",
    "    # Second pass: fill remaining slots fairly among classes that still have room\n",
    "    # Note: this is simple but effective; dataset is large so perf is fine.\n",
    "    while free_slots > 0:\n",
    "        progressed = False\n",
    "        for era, size in negative_sizes.items():\n",
    "            if free_slots <= 0:\n",
    "                break\n",
    "            if distribution[era] < size:\n",
    "                distribution[era] += 1\n",
    "                free_slots -= 1\n",
    "                progressed = True\n",
    "        if not progressed:\n",
    "            # No class can take more; break to avoid infinite loop\n",
    "            break\n",
    "\n",
    "    # Final neg_total might be slightly less than target if we ran out of data\n",
    "    final_neg_total = sum(distribution.values())\n",
    "    # To keep strict balance, also shrink pos_samples if needed\n",
    "    pos_samples = min(pos_samples, final_neg_total)\n",
    "\n",
    "    return {\n",
    "        \"pos_samples\": pos_samples,\n",
    "        \"neg_total\": final_neg_total,\n",
    "        \"neg_per_class\": distribution,\n",
    "    }\n",
    "\n",
    "\n",
    "for era in ALL_ERAS:\n",
    "    print(f\"\\n========================================\")\n",
    "    print(f\"BUILDING DATASET FOR ERA = {era}\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    era_folder = os.path.join(OUTPUT_DIR, era)\n",
    "    os.makedirs(era_folder, exist_ok=True)\n",
    "\n",
    "    # 1) POSITIVE samples (all rows where era == target)\n",
    "    pos_all = df[df[\"song_era\"] == era].copy()\n",
    "    num_pos = len(pos_all)\n",
    "    print(f\"Positive available: {num_pos}\")\n",
    "\n",
    "    # 2) NEGATIVE class sizes (per other era)\n",
    "    negative_sizes = {}\n",
    "    for other_era in ALL_ERAS:\n",
    "        if other_era == era:\n",
    "            continue\n",
    "        negative_sizes[other_era] = (df[\"song_era\"] == other_era).sum()\n",
    "\n",
    "    print(\"Negative sizes:\", negative_sizes)\n",
    "\n",
    "    # 3) Compute how many to sample (pos + per-negative-class)\n",
    "    sampling_plan = compute_balanced_samples(\n",
    "        pos_count=num_pos,\n",
    "        negative_sizes=negative_sizes,\n",
    "        max_total_limit=MAX_TOTAL_LIMIT,\n",
    "    )\n",
    "\n",
    "    pos_samples = sampling_plan[\"pos_samples\"]\n",
    "    neg_per_class = sampling_plan[\"neg_per_class\"]\n",
    "    neg_total = sampling_plan[\"neg_total\"]\n",
    "\n",
    "    print(f\"Sampling positives: {pos_samples}\")\n",
    "    print(f\"Total negatives: {neg_total}\")\n",
    "    print(f\"Negative per class: {neg_per_class}\")\n",
    "\n",
    "    if pos_samples == 0 or neg_total == 0:\n",
    "        print(f\"Skip era {era}: not enough data to build balanced dataset.\")\n",
    "        continue\n",
    "\n",
    "    # 4) Sample positives\n",
    "    pos_df = pos_all.sample(n=pos_samples, random_state=42)\n",
    "\n",
    "    # 5) Sample negatives per class using computed distribution\n",
    "    neg_dfs = []\n",
    "    for other_era, n_samples in neg_per_class.items():\n",
    "        if n_samples <= 0:\n",
    "            continue\n",
    "        subset = df[df[\"song_era\"] == other_era]\n",
    "        sampled = subset.sample(n=n_samples, random_state=42)\n",
    "        neg_dfs.append(sampled)\n",
    "\n",
    "    neg_df = pd.concat(neg_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Sampled positives: {len(pos_df)}\")\n",
    "    print(f\"Sampled negatives: {len(neg_df)}\")\n",
    "\n",
    "    # 6) Combine dataset\n",
    "    combined = pd.concat([pos_df, neg_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # 7) Add binary label\n",
    "    binary_col = f\"is_{era}\"\n",
    "    combined[binary_col] = (combined[\"song_era\"] == era).astype(int)\n",
    "\n",
    "    print(\"Final combined dataset size:\", len(combined))\n",
    "    print(\"Label balance:\\n\", combined[binary_col].value_counts())\n",
    "\n",
    "    # 8) Train/Val/Test split (stratified)\n",
    "    train_df, test_df = train_test_split(\n",
    "        combined,\n",
    "        test_size=0.10,\n",
    "        random_state=42,\n",
    "        stratify=combined[binary_col],\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=0.10,\n",
    "        random_state=42,\n",
    "        stratify=train_df[binary_col],\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "    # 9) Save datasets\n",
    "    train_df.to_csv(os.path.join(era_folder, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(era_folder, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(era_folder, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"\\nAll binary datasets created successfully with balanced pos/neg & fair per-era negatives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b16e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found binary dataset eras: ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 1970s\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 104264, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (104264, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.8388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.83     52132\n",
      "           1       0.81      0.88      0.85     52132\n",
      "\n",
      "    accuracy                           0.84    104264\n",
      "   macro avg       0.84      0.84      0.84    104264\n",
      "weighted avg       0.84      0.84      0.84    104264\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.7625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75      5792\n",
      "           1       0.74      0.81      0.77      5793\n",
      "\n",
      "    accuracy                           0.76     11585\n",
      "   macro avg       0.76      0.76      0.76     11585\n",
      "weighted avg       0.76      0.76      0.76     11585\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.7694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      6437\n",
      "           1       0.75      0.81      0.78      6436\n",
      "\n",
      "    accuracy                           0.77     12873\n",
      "   macro avg       0.77      0.77      0.77     12873\n",
      "weighted avg       0.77      0.77      0.77     12873\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/1970s\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 1980s\n",
      "========================================\n",
      "Training samples: 155466, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (155466, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77     77733\n",
      "           1       0.76      0.84      0.79     77733\n",
      "\n",
      "    accuracy                           0.78    155466\n",
      "   macro avg       0.79      0.78      0.78    155466\n",
      "weighted avg       0.79      0.78      0.78    155466\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6920\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.68      8637\n",
      "           1       0.67      0.74      0.71      8637\n",
      "\n",
      "    accuracy                           0.69     17274\n",
      "   macro avg       0.69      0.69      0.69     17274\n",
      "weighted avg       0.69      0.69      0.69     17274\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.67      9597\n",
      "           1       0.67      0.75      0.71      9597\n",
      "\n",
      "    accuracy                           0.69     19194\n",
      "   macro avg       0.69      0.69      0.69     19194\n",
      "weighted avg       0.69      0.69      0.69     19194\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/1980s\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 1990s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (324000, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.70      0.72    162000\n",
      "           1       0.72      0.76      0.74    162000\n",
      "\n",
      "    accuracy                           0.73    324000\n",
      "   macro avg       0.73      0.73      0.73    324000\n",
      "weighted avg       0.73      0.73      0.73    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63     18000\n",
      "           1       0.64      0.67      0.65     18000\n",
      "\n",
      "    accuracy                           0.64     36000\n",
      "   macro avg       0.64      0.64      0.64     36000\n",
      "weighted avg       0.64      0.64      0.64     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63     20000\n",
      "           1       0.64      0.67      0.65     20000\n",
      "\n",
      "    accuracy                           0.64     40000\n",
      "   macro avg       0.64      0.64      0.64     40000\n",
      "weighted avg       0.64      0.64      0.64     40000\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/1990s\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 2000s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (324000, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73    162000\n",
      "           1       0.73      0.75      0.74    162000\n",
      "\n",
      "    accuracy                           0.73    324000\n",
      "   macro avg       0.73      0.73      0.73    324000\n",
      "weighted avg       0.73      0.73      0.73    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.63      0.64     18000\n",
      "           1       0.64      0.66      0.65     18000\n",
      "\n",
      "    accuracy                           0.65     36000\n",
      "   macro avg       0.65      0.65      0.65     36000\n",
      "weighted avg       0.65      0.65      0.65     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.64     20000\n",
      "           1       0.65      0.66      0.65     20000\n",
      "\n",
      "    accuracy                           0.65     40000\n",
      "   macro avg       0.65      0.65      0.65     40000\n",
      "weighted avg       0.65      0.65      0.65     40000\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/2000s\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 2010s\n",
      "========================================\n",
      "Training samples: 220000, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (220000, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76    110072\n",
      "           1       0.76      0.74      0.75    109928\n",
      "\n",
      "    accuracy                           0.76    220000\n",
      "   macro avg       0.76      0.76      0.76    220000\n",
      "weighted avg       0.76      0.76      0.76    220000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68     18000\n",
      "           1       0.68      0.66      0.67     18000\n",
      "\n",
      "    accuracy                           0.67     36000\n",
      "   macro avg       0.67      0.67      0.67     36000\n",
      "weighted avg       0.67      0.67      0.67     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68     20000\n",
      "           1       0.68      0.66      0.67     20000\n",
      "\n",
      "    accuracy                           0.68     40000\n",
      "   macro avg       0.68      0.68      0.68     40000\n",
      "weighted avg       0.68      0.68      0.68     40000\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/2010s\n",
      "\n",
      "========================================\n",
      " TRAINING LOGISTIC REGRESSION FOR ERA = 2020s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Fitting TF-IDF...\n",
      "TF-IDF shape: (324000, 80000)\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.8308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83    162000\n",
      "           1       0.85      0.81      0.83    162000\n",
      "\n",
      "    accuracy                           0.83    324000\n",
      "   macro avg       0.83      0.83      0.83    324000\n",
      "weighted avg       0.83      0.83      0.83    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.7903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79     18000\n",
      "           1       0.80      0.77      0.79     18000\n",
      "\n",
      "    accuracy                           0.79     36000\n",
      "   macro avg       0.79      0.79      0.79     36000\n",
      "weighted avg       0.79      0.79      0.79     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.7926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80     20000\n",
      "           1       0.81      0.77      0.79     20000\n",
      "\n",
      "    accuracy                           0.79     40000\n",
      "   macro avg       0.79      0.79      0.79     40000\n",
      "weighted avg       0.79      0.79      0.79     40000\n",
      "\n",
      "Saved model & TF-IDF vectorizer â†’ ../logreg_binary_thunder/2020s\n",
      "\n",
      "ðŸŽ‰ All binary logistic regression models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# train_logreg_binary.py\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "BASE_DIR = \"../binary_datasets_thunder\"\n",
    "SAVE_DIR = \"../logreg_binary_thunder\"\n",
    "TEXT_COL = \"clean_lyrics\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "eras = sorted(os.listdir(BASE_DIR))\n",
    "print(\"Found binary dataset eras:\", eras)\n",
    "\n",
    "\n",
    "for era in eras:\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\" TRAINING LOGISTIC REGRESSION FOR ERA = {era}\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    era_folder = os.path.join(BASE_DIR, era)\n",
    "\n",
    "    # ---------- 1. LOAD DATA ----------\n",
    "    train_df = pd.read_csv(f\"{era_folder}/train.csv\")\n",
    "    val_df   = pd.read_csv(f\"{era_folder}/val.csv\")\n",
    "    test_df  = pd.read_csv(f\"{era_folder}/test.csv\")\n",
    "\n",
    "    bin_col = f\"is_{era}\"\n",
    "\n",
    "    y_train = train_df[bin_col].values\n",
    "    y_val   = val_df[bin_col].values\n",
    "    y_test  = test_df[bin_col].values\n",
    "\n",
    "    print(f\"Training samples: {len(train_df)}, Positive ratio: {train_df[bin_col].mean():.3f}\")\n",
    "\n",
    "    # ---------- 2. TF-IDF (fit on train only) ----------\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_features=80_000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "    )\n",
    "\n",
    "    print(\"Fitting TF-IDF...\")\n",
    "    X_train = vectorizer.fit_transform(train_df[TEXT_COL])\n",
    "    X_val   = vectorizer.transform(val_df[TEXT_COL])\n",
    "    X_test  = vectorizer.transform(test_df[TEXT_COL])\n",
    "\n",
    "    print(\"TF-IDF shape:\", X_train.shape)\n",
    "\n",
    "    # ---------- 3. Logistic Regression ----------\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1500,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"liblinear\",   # BEST for binary\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # ---------- 4. EVALUATION ----------\n",
    "    def eval_split(name, X, y):\n",
    "        preds = clf.predict(X)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        print(f\"\\n=== {name} ACCURACY: {acc:.4f}\")\n",
    "        print(classification_report(y, preds))\n",
    "        return acc\n",
    "\n",
    "    eval_split(\"TRAIN\", X_train, y_train)\n",
    "    eval_split(\"VAL\",   X_val,   y_val)\n",
    "    eval_split(\"TEST\",  X_test,  y_test)\n",
    "\n",
    "    # ---------- 5. SAVE MODEL ----------\n",
    "    save_path = f\"{SAVE_DIR}/{era}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    joblib.dump(clf, f\"{save_path}/logreg.joblib\")\n",
    "    joblib.dump(vectorizer, f\"{save_path}/tfidf.joblib\")\n",
    "\n",
    "    print(f\"Saved model & TF-IDF vectorizer â†’ {save_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All binary logistic regression models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53433579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /workspace/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 178ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/26.52 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 16.00 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 32.00 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 42.92 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 58.92 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 62.92 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 78.92 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 94.92 KiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 110.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 126.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 142.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 158.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 174.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 190.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 206.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 222.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 238.92 KiB/26.52 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.59 MiB/26.52 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 3.83 MiB/26.52 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 6.19 MiB/26.52 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 8.54 MiB/26.52 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 10.84 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 11.89 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 13.34 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 14.69 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)2m------------\u001b[0m\u001b[0m 15.83 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 16.98 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-\u001b[2m---------\u001b[0m\u001b[0m 18.12 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 19.28 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----\u001b[2m------\u001b[0m\u001b[0m 20.39 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 21.57 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)------\u001b[2m----\u001b[0m\u001b[0m 22.73 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------\u001b[2m---\u001b[0m\u001b[0m 23.85 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[2m-\u001b[0m\u001b[0m 25.01 MiB/26.52 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 921ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgensim\u001b[0m\u001b[2m==4.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b356b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found binary dataset eras: ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 1970s\n",
      "========================================\n",
      "Training samples: 104264, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 58832\n",
      "Embedding datasets...\n",
      "Embedding shape: (104264, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.76     52132\n",
      "           1       0.75      0.80      0.77     52132\n",
      "\n",
      "    accuracy                           0.76    104264\n",
      "   macro avg       0.76      0.76      0.76    104264\n",
      "weighted avg       0.76      0.76      0.76    104264\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.7538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      5792\n",
      "           1       0.73      0.79      0.76      5793\n",
      "\n",
      "    accuracy                           0.75     11585\n",
      "   macro avg       0.76      0.75      0.75     11585\n",
      "weighted avg       0.76      0.75      0.75     11585\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.7614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75      6437\n",
      "           1       0.75      0.79      0.77      6436\n",
      "\n",
      "    accuracy                           0.76     12873\n",
      "   macro avg       0.76      0.76      0.76     12873\n",
      "weighted avg       0.76      0.76      0.76     12873\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/1970s\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 1980s\n",
      "========================================\n",
      "Training samples: 155466, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 73423\n",
      "Embedding datasets...\n",
      "Embedding shape: (155466, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.6850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67     77733\n",
      "           1       0.67      0.73      0.70     77733\n",
      "\n",
      "    accuracy                           0.68    155466\n",
      "   macro avg       0.69      0.68      0.68    155466\n",
      "weighted avg       0.69      0.68      0.68    155466\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67      8637\n",
      "           1       0.67      0.72      0.70      8637\n",
      "\n",
      "    accuracy                           0.68     17274\n",
      "   macro avg       0.68      0.68      0.68     17274\n",
      "weighted avg       0.68      0.68      0.68     17274\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6780\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66      9597\n",
      "           1       0.66      0.72      0.69      9597\n",
      "\n",
      "    accuracy                           0.68     19194\n",
      "   macro avg       0.68      0.68      0.68     19194\n",
      "weighted avg       0.68      0.68      0.68     19194\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/1980s\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 1990s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 117765\n",
      "Embedding datasets...\n",
      "Embedding shape: (324000, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.6345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61    162000\n",
      "           1       0.62      0.69      0.65    162000\n",
      "\n",
      "    accuracy                           0.63    324000\n",
      "   macro avg       0.64      0.63      0.63    324000\n",
      "weighted avg       0.64      0.63      0.63    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.62     18000\n",
      "           1       0.62      0.69      0.65     18000\n",
      "\n",
      "    accuracy                           0.64     36000\n",
      "   macro avg       0.64      0.64      0.63     36000\n",
      "weighted avg       0.64      0.64      0.63     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6331\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61     20000\n",
      "           1       0.62      0.69      0.65     20000\n",
      "\n",
      "    accuracy                           0.63     40000\n",
      "   macro avg       0.63      0.63      0.63     40000\n",
      "weighted avg       0.63      0.63      0.63     40000\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/1990s\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 2000s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 115109\n",
      "Embedding datasets...\n",
      "Embedding shape: (324000, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.6428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63    162000\n",
      "           1       0.64      0.67      0.65    162000\n",
      "\n",
      "    accuracy                           0.64    324000\n",
      "   macro avg       0.64      0.64      0.64    324000\n",
      "weighted avg       0.64      0.64      0.64    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63     18000\n",
      "           1       0.63      0.67      0.65     18000\n",
      "\n",
      "    accuracy                           0.64     36000\n",
      "   macro avg       0.64      0.64      0.64     36000\n",
      "weighted avg       0.64      0.64      0.64     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6370\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63     20000\n",
      "           1       0.63      0.67      0.65     20000\n",
      "\n",
      "    accuracy                           0.64     40000\n",
      "   macro avg       0.64      0.64      0.64     40000\n",
      "weighted avg       0.64      0.64      0.64     40000\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/2000s\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 2010s\n",
      "========================================\n",
      "Training samples: 220000, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 103319\n",
      "Embedding datasets...\n",
      "Embedding shape: (220000, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.6755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67    110072\n",
      "           1       0.67      0.68      0.68    109928\n",
      "\n",
      "    accuracy                           0.68    220000\n",
      "   macro avg       0.68      0.68      0.68    220000\n",
      "weighted avg       0.68      0.68      0.68    220000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.6647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66     18000\n",
      "           1       0.66      0.67      0.67     18000\n",
      "\n",
      "    accuracy                           0.66     36000\n",
      "   macro avg       0.66      0.66      0.66     36000\n",
      "weighted avg       0.66      0.66      0.66     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.6737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67     20000\n",
      "           1       0.67      0.68      0.67     20000\n",
      "\n",
      "    accuracy                           0.67     40000\n",
      "   macro avg       0.67      0.67      0.67     40000\n",
      "weighted avg       0.67      0.67      0.67     40000\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/2010s\n",
      "\n",
      "========================================\n",
      " TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = 2020s\n",
      "========================================\n",
      "Training samples: 324000, Positive ratio: 0.500\n",
      "Training Word2Vec...\n",
      "Vocabulary size: 119996\n",
      "Embedding datasets...\n",
      "Embedding shape: (324000, 768)\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1305: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 47.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ACCURACY: 0.7874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79    162000\n",
      "           1       0.80      0.77      0.78    162000\n",
      "\n",
      "    accuracy                           0.79    324000\n",
      "   macro avg       0.79      0.79      0.79    324000\n",
      "weighted avg       0.79      0.79      0.79    324000\n",
      "\n",
      "\n",
      "=== VAL ACCURACY: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79     18000\n",
      "           1       0.79      0.77      0.78     18000\n",
      "\n",
      "    accuracy                           0.79     36000\n",
      "   macro avg       0.79      0.78      0.78     36000\n",
      "weighted avg       0.79      0.79      0.78     36000\n",
      "\n",
      "\n",
      "=== TEST ACCURACY: 0.7894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79     20000\n",
      "           1       0.80      0.78      0.79     20000\n",
      "\n",
      "    accuracy                           0.79     40000\n",
      "   macro avg       0.79      0.79      0.79     40000\n",
      "weighted avg       0.79      0.79      0.79     40000\n",
      "\n",
      "Saved LogisticRegression + Word2Vec â†’ ../logreg_binary_word2vec/2020s\n",
      "\n",
      "ðŸŽ‰ All Word2Vec + Logistic Regression binary models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# train_logreg_binary_word2vec.py\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = \"../binary_datasets_thunder\"\n",
    "SAVE_DIR = \"../logreg_binary_word2vec\"\n",
    "TEXT_COL = \"clean_lyrics\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "EMBED_DIM = 768\n",
    "WINDOW = 15\n",
    "MIN_COUNT = 5\n",
    "WORKERS = 32\n",
    "\n",
    "eras = sorted(os.listdir(BASE_DIR))\n",
    "print(\"Found binary dataset eras:\", eras)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# TOKENIZER\n",
    "# -------------------------------------------------\n",
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# SENTENCE EMBEDDING\n",
    "# -------------------------------------------------\n",
    "def sentence_embedding(tokens, wv):\n",
    "    vectors = [wv[word] for word in tokens if word in wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(EMBED_DIM)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def embed_corpus(list_tokenized, wv):\n",
    "    return np.vstack([sentence_embedding(tokens, wv) for tokens in list_tokenized])\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# TRAINING LOOP (ONE MODEL PER ERA)\n",
    "# -------------------------------------------------\n",
    "for era in eras:\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\" TRAINING WORD2VEC + LOGISTIC REGRESSION FOR ERA = {era}\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    era_folder = os.path.join(BASE_DIR, era)\n",
    "\n",
    "    # ---------- 1. LOAD ----------\n",
    "    train_df = pd.read_csv(f\"{era_folder}/train.csv\")\n",
    "    val_df   = pd.read_csv(f\"{era_folder}/val.csv\")\n",
    "    test_df  = pd.read_csv(f\"{era_folder}/test.csv\")\n",
    "\n",
    "    bin_col = f\"is_{era}\"\n",
    "\n",
    "    y_train = train_df[bin_col].values\n",
    "    y_val   = val_df[bin_col].values\n",
    "    y_test  = test_df[bin_col].values\n",
    "\n",
    "    print(f\"Training samples: {len(train_df)}, Positive ratio: {train_df[bin_col].mean():.3f}\")\n",
    "\n",
    "\n",
    "    # ---------- 2. Tokenize ----------\n",
    "    train_tokens = train_df[TEXT_COL].astype(str).apply(tokenize).tolist()\n",
    "    val_tokens   = val_df[TEXT_COL].astype(str).apply(tokenize).tolist()\n",
    "    test_tokens  = test_df[TEXT_COL].astype(str).apply(tokenize).tolist()\n",
    "\n",
    "\n",
    "    # ---------- 3. Train Word2Vec on TRAIN only ----------\n",
    "    print(\"Training Word2Vec...\")\n",
    "    w2v = Word2Vec(\n",
    "        sentences=train_tokens,\n",
    "        vector_size=EMBED_DIM,\n",
    "        window=WINDOW,\n",
    "        min_count=MIN_COUNT,\n",
    "        workers=WORKERS,\n",
    "    )\n",
    "    wv = w2v.wv\n",
    "\n",
    "    print(\"Vocabulary size:\", len(wv))\n",
    "\n",
    "\n",
    "    # ---------- 4. Convert to sentence embeddings ----------\n",
    "    print(\"Embedding datasets...\")\n",
    "    X_train = embed_corpus(train_tokens, wv)\n",
    "    X_val   = embed_corpus(val_tokens, wv)\n",
    "    X_test  = embed_corpus(test_tokens, wv)\n",
    "\n",
    "    print(\"Embedding shape:\", X_train.shape)\n",
    "\n",
    "\n",
    "    # ---------- 5. Logistic Regression ----------\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1500,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"liblinear\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # ---------- 6. Evaluation ----------\n",
    "    def eval_split(name, X, y):\n",
    "        preds = clf.predict(X)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        print(f\"\\n=== {name} ACCURACY: {acc:.4f}\")\n",
    "        print(classification_report(y, preds))\n",
    "        return acc\n",
    "\n",
    "    eval_split(\"TRAIN\", X_train, y_train)\n",
    "    eval_split(\"VAL\",   X_val,   y_val)\n",
    "    eval_split(\"TEST\",  X_test,  y_test)\n",
    "\n",
    "\n",
    "    # ---------- 7. Save ----------\n",
    "    save_path = f\"{SAVE_DIR}/{era}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    joblib.dump(clf, f\"{save_path}/logreg.joblib\")\n",
    "    w2v.save(f\"{save_path}/word2vec.model\")\n",
    "\n",
    "    print(f\"Saved LogisticRegression + Word2Vec â†’ {save_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All Word2Vec + Logistic Regression binary models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e39ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logreg_multiclass.py\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------\n",
    "TRAIN = \"../datasets_old/train_split.csv\"\n",
    "VAL   = \"../datasets_old/val_split.csv\"\n",
    "TEST  = \"../datasets_old/test_split.csv\"\n",
    "\n",
    "TEXT_COL = \"clean_lyrics\"\n",
    "LABEL_COL = \"labels\"\n",
    "\n",
    "SAVE_DIR = \"../logreg_multiclass\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------------------\n",
    "train_df = pd.read_csv(TRAIN)\n",
    "val_df   = pd.read_csv(VAL)\n",
    "test_df  = pd.read_csv(TEST)\n",
    "\n",
    "y_train = train_df[LABEL_COL].values\n",
    "y_val   = val_df[LABEL_COL].values\n",
    "y_test  = test_df[LABEL_COL].values\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:  \", len(val_df))\n",
    "print(\"Test size: \", len(test_df))\n",
    "print(\"Label classes:\", sorted(train_df[LABEL_COL].unique()))\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# TF-IDF\n",
    "# -----------------------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=120_000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF...\")\n",
    "X_train = vectorizer.fit_transform(train_df[TEXT_COL])\n",
    "X_val   = vectorizer.transform(val_df[TEXT_COL])\n",
    "X_test  = vectorizer.transform(test_df[TEXT_COL])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_train.shape)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# MULTI-CLASS LOGISTIC REGRESSION\n",
    "# -----------------------------------------\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",     # helps class imbalance\n",
    "    solver=\"liblinear\",          # supports OvR for multiclass\n",
    "    multi_class=\"ovr\",           # multi-class LR\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# EVALUATION\n",
    "# -----------------------------------------\n",
    "def evaluate(name, X, y):\n",
    "    preds = clf.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    print(f\"\\n=== {name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y, preds))\n",
    "    return acc\n",
    "\n",
    "evaluate(\"TRAIN\", X_train, y_train)\n",
    "evaluate(\"VAL\",   X_val,   y_val)\n",
    "evaluate(\"TEST\",  X_test,  y_test)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# SAVE MODEL + TF-IDF\n",
    "# -----------------------------------------\n",
    "joblib.dump(clf, f\"{SAVE_DIR}/logreg_multiclass.joblib\")\n",
    "joblib.dump(vectorizer, f\"{SAVE_DIR}/tfidf_multiclass.joblib\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Saved model & vectorizer in {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b71182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer...\n",
      "TF-IDF shape: (2613233, 120000)\n",
      "Training Ridge Regression model...\n",
      "\n",
      "=== TRAIN ===\n",
      "MAE: 6.020\n",
      "MSE: 70.026\n",
      "RMSE: 8.368\n",
      "RÂ²: 0.3346\n",
      "\n",
      "=== VAL ===\n",
      "MAE: 6.264\n",
      "MSE: 75.471\n",
      "RMSE: 8.687\n",
      "RÂ²: 0.2800\n",
      "\n",
      "=== TEST ===\n",
      "MAE: 6.265\n",
      "MSE: 75.717\n",
      "RMSE: 8.702\n",
      "RÂ²: 0.2824\n",
      "\n",
      "Model saved â†’ ../regression_year_model\n"
     ]
    }
   ],
   "source": [
    "# train_year_regression.py\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------\n",
    "TEXT_COL = \"clean_lyrics\"\n",
    "LABEL_COL = \"labels\"    # numeric year already\n",
    "DATA_DIR = \"../datasets_old\"\n",
    "SAVE_DIR = \"../regression_year_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------------------\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train_split.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_DIR}/val_split.csv\")\n",
    "test_df  = pd.read_csv(f\"{DATA_DIR}/test_split.csv\")\n",
    "\n",
    "y_train = train_df[LABEL_COL].values\n",
    "y_val   = val_df[LABEL_COL].values\n",
    "y_test  = test_df[LABEL_COL].values\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# TF-IDF (fit only on train)\n",
    "# ------------------------------------------\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=120_000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df[TEXT_COL])\n",
    "X_val   = vectorizer.transform(val_df[TEXT_COL])\n",
    "X_test  = vectorizer.transform(test_df[TEXT_COL])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_train.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# REGRESSION MODEL (Ridge)\n",
    "# ------------------------------------------\n",
    "print(\"Training Ridge Regression model...\")\n",
    "\n",
    "model = Ridge(\n",
    "    alpha=1.0,       # L2 regularized (best for TF-IDF)\n",
    "    max_iter=3000,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# EVALUATION FUNCTION\n",
    "# ------------------------------------------\n",
    "def evaluate(name, X, y):\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    r2  = r2_score(y, preds)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mse):.3f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# REPORT METRICS\n",
    "# ------------------------------------------\n",
    "evaluate(\"TRAIN\", X_train, y_train)\n",
    "evaluate(\"VAL\",   X_val,   y_val)\n",
    "evaluate(\"TEST\",  X_test,  y_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# SAVE MODEL + TF-IDF\n",
    "# ------------------------------------------\n",
    "joblib.dump(model, f\"{SAVE_DIR}/ridge_regressor.joblib\")\n",
    "joblib.dump(vectorizer, f\"{SAVE_DIR}/tfidf.joblib\")\n",
    "\n",
    "print(\"\\nModel saved â†’\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cedda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
